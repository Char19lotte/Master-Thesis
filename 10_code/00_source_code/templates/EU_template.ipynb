{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Data Collection - European Parliament*\n",
    "## Preparing Raw Data\n",
    "---\n",
    "**Sample Text 2**\n",
    "Title: Democratic scrutiny of social media and the protection of fundamental rights <br>\n",
    "Date: February 10, 2021 - Brussels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib import request\n",
    "from __future__ import division\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "import os.path \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Process: Trimming debate by inserting the original English or translated English files and tokenizing them.\n",
    "*Note*: Due to time constraint, the process has been optimized.\n",
    "\n",
    "- English parts of the debate will be added manually as a string and then tokenized. \n",
    "\n",
    "- A consistent method of translating and then adding will be applied to all EU Parliament debates:  Non-English parts are copied from the original web pages, inserted in the consistent choice of translation tool, Google Translate (https://translate.google.com/?hl=de&tab=TT), translated to English and pasted in as a string. \n",
    "\n",
    "- Afterwards, the same steps are applied as per usual (tokenizing, standardizing).\n",
    "\n",
    "Because of the changed process, the URL and step of webscraping are technically no longer necessary, will however be included for the purpose of completeness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.europarl.europa.eu/doceo/document/CRE-9-2021-02-10-ITM-008_EN.html\"\n",
    "# html = requests.get(url)\n",
    "# raw = BeautifulSoup(html.content, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_1 = ''\n",
    "tokens1_1 = word_tokenize(raw1_1)\n",
    "for word in tokens1_1:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_2 = ''\n",
    "tokens1_2 = word_tokenize(raw1_2)\n",
    "for word in tokens1_2:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_3 = ''\n",
    "tokens1_3 = word_tokenize(raw1_3)\n",
    "for word in tokens1_3:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_4 = ''\n",
    "tokens1_4 = word_tokenize(raw1_4)\n",
    "for word in tokens1_4:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_5 = ''\n",
    "tokens1_5 = word_tokenize(raw1_5)\n",
    "for word in tokens1_5:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_6 = ''\n",
    "tokens1_6 = word_tokenize(raw1_6)\n",
    "for word in tokens1_6:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_7 = ''\n",
    "tokens1_7 = word_tokenize(raw1_7)\n",
    "for word in tokens1_7:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_8 = ''\n",
    "tokens1_8 = word_tokenize(raw1_8)\n",
    "for word in tokens1_8:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_9 = ''\n",
    "tokens1_9 = word_tokenize(raw1_9)\n",
    "for word in tokens1_9:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_10 = ''\n",
    "tokens1_10 = word_tokenize(raw1_10)\n",
    "for word in tokens1_10:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_11 = ''\n",
    "tokens1_11 = word_tokenize(raw1_11)\n",
    "for word in tokens1_11:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_12 = ''\n",
    "tokens1_12 = word_tokenize(raw1_12)\n",
    "for word in tokens1_12:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_13 = ''\n",
    "tokens1_13 = word_tokenize(raw1_13)\n",
    "for word in tokens1_13:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_14 = ''\n",
    "tokens1_14 = word_tokenize(raw1_14)\n",
    "for word in tokens1_14:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_15 = ''\n",
    "tokens1_15 = word_tokenize(raw1_15)\n",
    "for word in tokens1_15:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_16 = ''\n",
    "tokens1_16 = word_tokenize(raw1_16)\n",
    "for word in tokens1_16:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_17 = ''\n",
    "tokens1_17 = word_tokenize(raw1_17)\n",
    "for word in tokens1_17:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_18 = ''\n",
    "tokens1_18 = word_tokenize(raw1_18)\n",
    "for word in tokens1_18:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_19 = ''\n",
    "tokens1_19 = word_tokenize(raw1_19)\n",
    "for word in tokens1_19:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_20 = ''\n",
    "tokens1_20 = word_tokenize(raw1_20)\n",
    "for word in tokens1_20:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_21 = ''\n",
    "tokens1_21 = word_tokenize(raw1_21)\n",
    "for word in tokens1_21:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw1_22 = ''\n",
    "tokens1_1 = word_tokenize(raw1_22)\n",
    "tokens1_1\n",
    "for word in tokens1_1:\n",
    "    print(word, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Combine all parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens1_1 + tokens1_2 + tokens1_3 + tokens1_4 + tokens1_5 + tokens1_6 + tokens1_7 + tokens1_8 + tokens1_9 + tokens1_10 + tokens1_11 + tokens1_12 + tokens1_13 + tokens1_14 + tokens1_15 + tokens1_16 + tokens1_17 + tokens1_18 + tokens1_19 + tokens1_20 + tokens1_21 + tokens1_22 + tokens1_23 + tokens1_24 + tokens1_25 + tokens1_26 + tokens1_27 + tokens1_28 + tokens1_29 + tokens1_30 + tokens1_31 + tokens1_32 + tokens1_33 + tokens1_34 + tokens1_35 + tokens1_36 + tokens1_37 + tokens1_38 + tokens1_39 + tokens1_40 + tokens1_41 + tokens1_42 + tokens1_43 + tokens1_44 + tokens1_45 + tokens1_46 + tokens1_47 + tokens1_48 + tokens1_49 + tokens1_50 + tokens1_51 + tokens1_52 + tokens1_53 + tokens1_54 + tokens1_55 + tokens1_56 + tokens1_57 + tokens1_58 + tokens1_59 + tokens1_60 + tokens1_61 + tokens1_62 + tokens1_63 + tokens1_64 + tokens1_65 + tokens1_66 + tokens1_67 + tokens1_68 + tokens1_69 + tokens1_70 + tokens1_71 + tokens1_72 + tokens1_73 + tokens1_74 + tokens1_75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Normalize the words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokens)\n",
    "eutext02 = [w.lower() for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Save Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/Users/charlottekaiser/Documents/uni/Hertie/master_thesis/20_intermediate_files'\n",
    "file_name = \"EU02_Democratic scrutiny of social media and the protection of fundamental rights.txt\"\n",
    "completeName = os.path.join(save_path, file_name)\n",
    "output = open(completeName, 'w')\n",
    "print(eutext02, file=output)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9869976cf380d12cb70e759e57434a8e82bae01a9f74e734956416b40621c64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
