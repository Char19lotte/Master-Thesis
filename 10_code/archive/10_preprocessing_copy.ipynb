{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Data Analysis*\n",
    "##  Pre-processing\n",
    "---\n",
    "Goal: \n",
    "- process the relevant debate text files and create a dataframe with level (EU = 1, US =0), debate title and text\n",
    "- conduct preprocessing steps such as lemmatizing, get rid of punctuations,  <br> <br>\n",
    "*conducted in March 2022*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/charlottekaiser/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk, re, pprint\n",
    "import json\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "import nltk.data\n",
    "import os.path \n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import PlaintextCorpusReader \n",
    "from nltk.app import concordance\n",
    "from nltk.corpus import BracketParseCorpusReader\n",
    "import numpy as np\n",
    "import contractions\n",
    "import statsmodels.formula.api as smf\n",
    "import altair as alt\n",
    "import tmtoolkit\n",
    "import spacy as spacy\n",
    "import logging, warnings\n",
    "from tmtoolkit.corpus import Corpus\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "nltk.download('omw-1.4')\n",
    "import pickle\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Prerequisites and pre-processing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/charlottekaiser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sample specific stop words that are redundant and have no substantial relevance; also add words that are project-specific stopwords\n",
    "stopwords.add('president')\n",
    "stopwords.add('mr')\n",
    "stopwords.add('ms')\n",
    "stopwords.add('commission')\n",
    "stopwords.add('congress')\n",
    "stopwords.add('speaker')\n",
    "stopwords.add('also')\n",
    "stopwords.add('artificial')\n",
    "stopwords.add('intelligence')\n",
    "stopwords.add('digital')\n",
    "stopwords.add('ai')\n",
    "stopwords.add('pro')\n",
    "stopwords.add('tempore')\n",
    "stopwords.add('representative')\n",
    "stopwords.add('thank')\n",
    "stopwords.add('dear')\n",
    "stopwords.add('rapporteur')\n",
    "stopwords.add('lady')\n",
    "stopwords.add('committee')\n",
    "stopwords.add('report')\n",
    "stopwords.add('legislation')\n",
    "stopwords.add('like')\n",
    "stopwords.add('subcommittee')\n",
    "stopwords.add('gentleman')\n",
    "stopwords.add('r')\n",
    "stopwords.add('colleague')\n",
    "stopwords.add('madam')\n",
    "stopwords.add('ha')\n",
    "stopwords.add('wa')\n",
    "stopwords.add('for')\n",
    "stopwords.add('in')\n",
    "stopwords.add('-')\n",
    "stopwords.add(',')\n",
    "stopwords.add('and')\n",
    "stopwords.add('house')\n",
    "stopwords.add('chairwoman')\n",
    "stopwords.add('sponsor')\n",
    "stopwords.add('gentlewoman')\n",
    "stopwords.add('verts')\n",
    "stopwords.add('renew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/charlottekaiser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Define new function\n",
    "# NLTKâ€™s Wordnet stores meanings of words, synonyms, antonyms, etc. - for ref, see: https://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html \n",
    "# WordNetLemmatizer gets the root, for ref, see: https://www.nltk.org/_modules/nltk/stem/wordnet.html\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenizer for nltk using RegexpTokenizer, to keep tokens that are alphanumeric characters, get rid off punctuation\n",
    "tokenizer = RegexpTokenizer(r'\\w+') \n",
    "# Define a noun tagger \n",
    "is_noun = lambda pos: pos[:2] == 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess for LDA\n",
    "def prepare_for_lda(text):\n",
    "    text = ''.join(c for c in text if not c.isdigit())\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    # tokens = [word for word,pos in tags if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')] #keep only nouns\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/charlottekaiser/Documents/uni/Hertie/master_thesis/00_data/50_analysis')\n",
    "with open('raw_corpus_aggregate.csv') as f:\n",
    "    df = pd.read_csv(f)\n",
    "f.close()\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df:\n",
    "    text = row[3:]\n",
    "    text = prepare_for_lda(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['without_stopwords'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eu</th>\n",
       "      <th>debate</th>\n",
       "      <th>text</th>\n",
       "      <th>without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EU02_Democratic scrutiny of social media and t...</td>\n",
       "      <td>['ana', 'paula', 'zacarias', ',', 'president-i...</td>\n",
       "      <td>['ana', 'paula', 'zacarias', ',', 'president-i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>EU03_European strategy for data - Commission e...</td>\n",
       "      <td>['henna', 'virkkunen', '(', 'ppe', ')', '.', '...</td>\n",
       "      <td>['henna', 'virkkunen', '(', 'ppe', ')', '.', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>EU11_Digital Europe programme.txt</td>\n",
       "      <td>['valter', 'flego', ',', 'reporter', '.', '-',...</td>\n",
       "      <td>['valter', 'flego', ',', 'reporter', '.', '-',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>EU13_Artificial intelligence in education, cul...</td>\n",
       "      <td>['sabine', 'verheyen', ',', 'rapporteur', '.',...</td>\n",
       "      <td>['sabine', 'verheyen', ',', 'rapporteur', '.',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>EU14_Digital future of Europe- digital single ...</td>\n",
       "      <td>['deirdre', 'clune', ',', 'rapporteur', '.', '...</td>\n",
       "      <td>['deirdre', 'clune', ',', 'rapporteur', '.', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eu                                             debate  \\\n",
       "0   1  EU02_Democratic scrutiny of social media and t...   \n",
       "1   1  EU03_European strategy for data - Commission e...   \n",
       "2   1                  EU11_Digital Europe programme.txt   \n",
       "3   1  EU13_Artificial intelligence in education, cul...   \n",
       "4   1  EU14_Digital future of Europe- digital single ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  ['ana', 'paula', 'zacarias', ',', 'president-i...   \n",
       "1  ['henna', 'virkkunen', '(', 'ppe', ')', '.', '...   \n",
       "2  ['valter', 'flego', ',', 'reporter', '.', '-',...   \n",
       "3  ['sabine', 'verheyen', ',', 'rapporteur', '.',...   \n",
       "4  ['deirdre', 'clune', ',', 'rapporteur', '.', '...   \n",
       "\n",
       "                                   without_stopwords  \n",
       "0  ['ana', 'paula', 'zacarias', ',', 'president-i...  \n",
       "1  ['henna', 'virkkunen', '(', 'ppe', ')', '.', '...  \n",
       "2  ['valter', 'flego', ',', 'reporter', '.', '-',...  \n",
       "3  ['sabine', 'verheyen', ',', 'rapporteur', '.',...  \n",
       "4  ['deirdre', 'clune', ',', 'rapporteur', '.', '...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataframe for further analysis\n",
    "os.chdir('/Users/charlottekaiser/Documents/uni/Hertie/master_thesis/00_data/50_analysis')\n",
    "df.to_csv('ready_corpus_aggregate.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9869976cf380d12cb70e759e57434a8e82bae01a9f74e734956416b40621c64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
